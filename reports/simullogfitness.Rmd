---
title: "Simulation of log fitness change"
author: "Laure Olazcuaga, Nicolas Rode"
date: "`r format(Sys.Date(), '%d-%B-%Y')`"
output: 
  html_document:
    theme: "journal"
    toc: true
    toc_depth: 3
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
devtools::load_all()
```

# Import data
```{r }
## Initial phenotyping
data_G0 <- loadfitnessdata(dataset = "Selection_Phenotypage_G0_G7_G8.csv", generation = "G1")

## Intemediate phenotyping
data_G7 <- loadfitnessdata(dataset = "Selection_Phenotypage_G0_G7_G8.csv", generation = "G7")

## Final phenotyping
data_G29 <- loadfitnessdata(dataset = "PERFORMANCE_Comptage_adultes_G13G14G15G16G17G18G19G20G21G22G23G24G25G26G27G28G29.csv", generation = "29")

head(data_G0)
head(data_G7)
head(data_G29)

```

# Choose the best distribution to describe the data
## Estimate parameters using negative binomial
```{r }

tapply(data_G0$Nb_adults, data_G0$Treatment, var)/tapply(data_G0$Nb_adults, data_G0$Treatment, mean)

## Fit model
mnegbin <- MASS::glm.nb(Nb_adults~1, data=data_G0)
summary(mnegbin)

## Estimation of the mean
c(observed=mean(data_G0$Nb_adults), fitted=exp(as.numeric(coef(mnegbin))))

## Estimation of the variance
c(observed=var(data_G0$Nb_adults), fitted=exp(coef(mnegbin))+(exp(coef(mnegbin))^2)/mnegbin$theta)

## Goodness of fit using fitdistrplus pacjage
fitnegbin <- fitdistrplus::fitdist(data_G0$Nb_adults, "nbinom")
plot(fitnegbin)




```

## Estimate parameters using Poisson lognormal
```{r }
data_G0$Obs <- as.factor(1:nrow(data_G0))

mpoislognormal <- lme4::glmer(Nb_adults ~ 1 + (1|Obs), data=data_G0, family="poisson")
summary(mpoislognormal)
```

## Compare Negative binomial and Poisson lognormal
```{r }

## Simulate data with Negative Binomial distribution
x.teo.negbin <- MASS::rnegbin(n=nrow(data_G0), mu=exp(coef(mnegbin)), theta=mnegbin$theta)
x.teo.negbin <- rnbinom(n=nrow(data_G0), size=mnegbin$theta, mu=exp(coef(mnegbin)))

## Simulate data with Poission lognormal distribution
x.teo.poislognormal <- unlist(simulate(mpoislognormal))

## Plot observed and expected density distributions
plot(prop.table(table(data_G0$Nb_adults)), ylab="Frequency")
lines(0:70, dnbinom(0:70, mu=exp(coef(mnegbin)), size=mnegbin$theta), col = "red")

## Plot empirical and expected cumulative distributions
plot(ecdf(data_G0$Nb_adults), xlab="Data", ylab="CDF", main="", las=1)
lines(0:70, pnbinom(0:70, mu=exp(coef(mnegbin)), size=mnegbin$theta), col = "red", lwd = 2, type = "s")
## Add legend
legend(0, 0.7, legend=c("Empirical", "Theoretical"), col=c("black", "red"), lwd=2, bty="n")

## QQplot to compared Negative binomial and Poisson log normal distributions
qqplot(data_G0$Nb_adults, x.teo.negbin, main="QQ-plot", xlab="Observed data", ylab="Simulated data", las=1, xlim = c(0, 100), ylim = c(0, 100)) ## QQ-plot
points(sort(data_G0$Nb_adults), sort(x.teo.poislognormal), pch=1, col='red')
abline(0,1)
## Add legend
legend(0, 70, legend=c("Neg. Bin.", "Pois. Lognormal"), col=c("black", "red"), pch=1, bty="n")


## Compare AIC
AIC(mnegbin, mpoislognormal)

## Cl: The fit of the Negative binomial is better


```

# Compare estimates
## Compare estimates per fruit
```{r }

## Estimate parameters for each generation and each fruit
overdispG0 <- as.data.frame(t(sapply(levels(data_G0$Treatment), estim_overdisp, colfactor="Treatment", data=data_G0, generation="G0")))
overdispG7 <- as.data.frame(t(sapply(levels(data_G7$Treatment), estim_overdisp, colfactor="Treatment", data=data_G7[data_G7$SA==1,], generation="G7")))
overdispG29 <- as.data.frame(t(sapply(levels(data_G29$Treatment), estim_overdisp, colfactor="Treatment", data=data_G29[data_G29$SA==1,], generation="G29")))

## Combine datasets
overdisp <- rbind(overdispG0, overdispG7, overdispG29)
colnames(overdisp) <- c("fruit", "generation", "number_tubes_counted", "fitted_mean_nb_adults", "fitted_theta", "observed_var_nb_adults", "fitted_var_nb_adults", "obsoverdisp", "fittedoverdisp", "standardized_mean", "standardized_mean_small_sample")

rownames(overdisp) <- 1:nrow(overdisp)

## Sort final dataset
overdisp[order(overdisp$fruit),]
```

## Compare estimates per experimental population
```{r }

## Estimate parameters for each generation and each population
overdispG0 <- as.data.frame(t(sapply(levels(data_G0$Treatment), estim_overdisp, colfactor="Treatment", data=data_G0, generation="G0")))
overdispG7 <- as.data.frame(t(sapply(levels(data_G7$Line), estim_overdisp, colfactor="Line", data=data_G7[data_G7$SA==1,], generation="G7")))
overdispG29 <- as.data.frame(t(sapply(levels(data_G29$Line), estim_overdisp, colfactor="Line", data=data_G29[data_G29$SA==1,], generation="G29")))

## Combine datasets
overdisp <- rbind(overdispG0, overdispG7, overdispG29)
colnames(overdisp) <- c("factor", "generation", "number_tubes_counted", "fitted_mean_nb_adults", "fitted_theta", "observed_var_nb_adults", "fitted_var_nb_adults", "obsoverdisp", "fittedoverdisp", "standardized_mean", "standardized_mean_small_sample")

rownames(overdisp) <- 1:nrow(overdisp)

overdisp <- data.frame(fruit=as.factor(substr(overdisp$factor, 1, 2)), overdisp)
levels(overdisp$fruit) <- c("Cherry", "Cherry", "Cranberry", "Cranberry", "Strawberry", "Strawberry")

## Sort final dataset
overdisp[order(overdisp$fruit, overdisp$generation, overdisp$fitted_mean_nb_adults),]

## Only populations CE2 and CR2 do not pass Geary's test (the threshold of a standardized mean greater than 3)

```



# Fitness estimation
## Simulations
```{r }
N_ancestor=25
ntubes_ancestor=30
theta_ancestor=3 #median(as.numeric(as.character(overdisp$fitted_theta)))

computemeanvar_fitnessdata(seed=1, ntubes=ntubes_ancestor, N=N_ancestor, theta=theta_ancestor)



```

## Check the reliability of the bootstrap procedure
```{r }
sim <- data.frame(t(sapply(1:1000, computemeanvar_fitnessdata, ntubes=ntubes_ancestor, N=N_ancestor, theta=theta_ancestor)))

## Variance estimates ~ OK
hist(sim$deltamethod_varestimfitness, xlab="Variance estimate based on the observed mean and variance (delta method approx)")
abline(v=sim$deltamethod_varfitness, col="red")

## 95% confidence interval with delta method is OK
mean(sim$indicboot)
mean(sim$indicnormaprox)

## Check the influence of the number of tubes on power
Ntube <- seq(4, 100, by=10)

bias <- c()
powerboot <- c()
powernormapprox <- c()
for (ntubes_ancestor in Ntube){
  sim <- data.frame(t(sapply(1:500, computemeanvar_fitnessdata, ntubes=ntubes_ancestor, N=N_ancestor, theta=theta_ancestor)))
  bias <- c(bias, mean(sim$expected_meanfitness-sim$observed_meanfitness))
  powerboot <- c(powerboot, mean(sim$indicboot))
  powernormapprox <- c(powernormapprox, mean(sim$indicnormaprox))
}

## Positive bias decreases with the number of tubes
bias

plot(Ntube, powerboot, xlab="Number of tubes", ylab="Power", ylim=c(0, 1), las=1, type="l", xaxt='n', lty=2, bty="n")
lines(Ntube, powernormapprox, col="black", lty=1)
axis(1, at=seq(0, 100, by=10))
abline(h=0.95, col="red", lwd=2)
legend(0, 0.5, legend=c( "Normal approximation", "Boostrapping"), bty="n", lwd=1, lty=c(1, 2))
## 95% CI computed with -/+ 1.96 SE is OK
```

# Fitness difference estimation
## Simulation
```{r }
## Formula for the computation
#ln(a/b)-ln(c/d)=ln(a*d/b*c)
#ln(a/20)-ln(c/20)=ln(a*20/c*20)=ln(a/c)=ln(a)-ln(c)

## Compute the fitness difference for one seed
computemeanvar_fitnessdifferencedata(seed=1, ntubes_ancestor=30, N_ancestor=25, theta_ancestor=3, ntubes_derived=30, N_derived=30, theta_derived=3)

```


## Check the reliability of the bootstrap procedure
```{r }
## Check the function for a set of realistic parameters
sim <- data.frame(t(sapply(1:1000, computemeanvar_fitnessdifferencedata, ntubes_ancestor=30, N_ancestor=25, theta_ancestor=3, ntubes_derived=30, N_derived=30, theta_derived=3)))

## Variance estimates ~ OK
hist(sim$boot_varestimfitnessdiff, xlab="Variance estimate based on boostrap")
abline(v=sim$deltamethod_varfitnessdiff, col="red")

## 95% confidence interval is actually a 90% CI
mean(sim$indicboot)
mean(sim$indicnormapprox)

## Check the influence of the number of tubes on power
Ntube <- seq(4, 100, by=10)

## Initialize vector for bias and power
bias <- c()
powerboot <- c()
powernormapprox <- c()

for (ntubes_tested in Ntube){
  sim <- data.frame(t(sapply(1:500, computemeanvar_fitnessdifferencedata, ntubes_ancestor=100, N_ancestor=25, theta_ancestor=3, ntubes_derived=ntubes_tested, N_derived=30, theta_derived=3)))
  bias <- c(bias, mean(sim$expected_meanfitnessdiff-sim$observed_meanfitnessdiff))
  powerboot <- c(powerboot, mean(sim$indicboot))
  powernormapprox <- c(powernormapprox, mean(sim$indicnormapprox))
}

## No bias
bias

plot(Ntube, powerboot, xlab="Number of tubes", ylab="Power", ylim=c(0, 1), las=1, type="l", xaxt='n', lty=2)
lines(Ntube, powernormapprox)
axis(1, at=seq(0, 100, by=10))
abline(h=0.95, col="red", lwd=2)
legend(0, 0.5, legend=c( "Normal approximation", "Boostrapping"), bty="n", lwd=1, lty=c(1, 2))
## 95% CI should be computed with c(0.025*delta, 0.975+0.025*(1-delta))

```
